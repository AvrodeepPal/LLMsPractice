Here are **detailed notes on Causal Attention**, covering its purpose, mechanism, masking strategy, and comparison to general self-attention. This is especially relevant in autoregressive models like GPT.

---

## ğŸ” **Causal Attention: Core Concept**

---

### ğŸ§  What is Causal Attention?

**Causal attention** is a variant of **self-attention** where each token can only attend to itself and **previous tokens**, not future ones. It's also called **masked self-attention** or **autoregressive attention**.

This constraint enforces **temporal causality** â€” meaning:
> A prediction at time step `t` should **only be influenced** by tokens at time steps `â‰¤ t`.

---

## ğŸ¤– Why Causal Attention â‰  Regular Self-Attention

### ğŸ”„ **Self-Attention**:
- Every token **attends to all tokens** (past, present, future).
- Used in **bidirectional** models like **BERT**.
- Great for **understanding** (e.g., classification, Q&A).

### â³ **Causal (Masked) Attention**:
- Every token attends to **itself and past tokens only**.
- Used in **autoregressive generation**, like **GPT**.
- Great for **generation** (e.g., text completion, forecasting).

> If you're trying to generate "The cat is...", the word "cat" shouldn't know about "is" while generating.

---

## ğŸ”’ **Why Masking?**

To **prevent data leakage** from the future during training.

### ğŸ” Mechanism:
We use a **mask** (typically upper triangular) to disallow attention to future tokens.

### âš™ï¸ Implementation:
```python
# Mask upper triangle
mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) * -inf
masked_scores = attn_scores + mask  # applies -inf to future tokens
attn_weights = softmax(masked_scores, dim=-1)
```

This results in:

- ğŸŸ¥ Future positions getting **-inf**
- â¬‡ï¸ `softmax(-inf) = 0` â†’ **no attention to the future**

---

## ğŸ§® **Step-by-Step: Causal Attention Flow**

1. **Input tensor** `x` â†’ project into `queries`, `keys`, `values`
2. **Raw Attention Scores**:
   \[
   \text{attn\_scores} = QK^T
   \]
3. **Apply causal mask** to `attn_scores`:
   \[
   \text{masked\_scores}_{i,j} =
   \begin{cases}
   \text{attn\_scores}_{i,j} & \text{if } j \leq i \\
   -\infty & \text{if } j > i
   \end{cases}
   \]
4. **Softmax** turns scores into probabilities:
   \[
   \text{attn\_weights} = \text{softmax}(\text{masked\_scores})
   \]
5. **Compute Context Vector**:
   \[
   \text{context} = \text{attn\_weights} \cdot V
   \]

---

## ğŸ“ˆ **How Masking Helps**

| ğŸ§  Problem | ğŸš€ Solution via Masking |
|-----------|--------------------------|
| **Future info leakage** | Prevents target token from seeing future during training |
| **Temporal causality violation** | Enforces left-to-right structure |
| **Autoregressive decoding** | Enables generation one step at a time |
| **Training vs inference mismatch** | Mask ensures same behavior in both |

---

## ğŸ” Visual Explanation of Causal Mask (Triangular)

Given `seq_len = 4`, the mask matrix looks like:

\[
\text{Mask} = \begin{bmatrix}
0 & -\infty & -\infty & -\infty \\
0 & 0 & -\infty & -\infty \\
0 & 0 & 0 & -\infty \\
0 & 0 & 0 & 0 \\
\end{bmatrix}
\]

After softmax:
- Only the lower triangle values get non-zero probabilities.
- Future tokens are completely ignored.

---

## âš”ï¸ Causal Attention vs Previous Methods

| Feature | RNN/LSTM | Self-Attention | Causal Attention |
|--------|----------|----------------|------------------|
| Sequence Order | Preserved | Not by default | Preserved via mask |
| Parallelizable | âŒ No | âœ… Yes | âœ… Yes |
| Causal | âœ… Yes | âŒ No | âœ… Yes |
| Used In | RNNs | BERT | GPT, GPT-2, GPT-3 |

---

## âœ… Summary

| âœ… Term | ğŸ’¡ Meaning |
|--------|-----------|
| **Causal attention** | Self-attention that only looks backward |
| **Masking** | Prevents future token attention |
| **Upper triangular -âˆ** | Removes influence of future in softmax |
| **Autoregressive** | Each token depends only on past + present |
| **Used in** | Language generation (GPT), forecasting, chatbots |

---

Would you like a visual attention heatmap demo or masked attention example in Colab?
