Here is a complete **theoretical write-up** that ties together all the concepts we discussed around **context vectors in attention mechanisms**, suitable for use in notes, reports, or documentation.

---

# üìò Theory: The Attention Mechanism and Context Vector

---

## üåê Introduction to Attention

In sequence models like transformers, **attention** is a technique that allows each token to **look at other tokens** in the input sequence and gather **relevant contextual information**. The output of attention is a **context vector** for each token ‚Äî an enriched embedding that captures both its own meaning and the meaning of surrounding tokens.

This mechanism is essential in tasks like translation, summarization, and language understanding.

---

## üìå 1. What is a Context Vector?

A **context vector** is the output vector generated for each input token after applying the attention mechanism. It is a **weighted sum** of value vectors (`V`) of all tokens in the sequence, where the weights are calculated based on the **relevance** of other tokens to the current token.

The context vector answers:
> "Given my position in the sentence, what should I focus on, and how should I represent myself with that context?"

This allows each token to carry **global context**, not just local information.

---

## üß† 2. Components of Attention: Q, K, V

Each input token embedding `x` is projected into three different roles using **learnable linear transformations** (i.e., weight matrices):

| Component | Equation | Role | Intuition |
|----------|----------|------|-----------|
| **Query (Q)** | `Q = X @ Wq` | What I‚Äôm looking for | Like a question vector |
| **Key (K)** | `K = X @ Wk` | What I contain | Like a label describing a token |
| **Value (V)** | `V = X @ Wv` | What I can offer | Carries actual content/information |

Where:
- `X` is the matrix of token embeddings of shape `[n_tokens √ó d_model]`
- `Wq`, `Wk`, `Wv` are trainable matrices of shape `[d_model √ó d_k or d_v]`

---

## üßÆ 3. Attention Score: `Q @ K·µÄ`

To compute how much **each token should attend to others**, we calculate:

```
attention_scores = Q @ K·µÄ
```

This gives a **similarity matrix** of shape `[n_tokens √ó n_tokens]`, where:

- Entry `[i][j]` is the dot product between the **query** of token `i` and **key** of token `j`
- It measures **how relevant token `j` is to token `i`**

### Example
If token "sat" attends strongly to "cat", then:
```
Q["sat"] ‚Ä¢ K["cat"] ‚Üí high score
```

---

## ‚öñÔ∏è 4. Scaling the Attention Scores: `/ sqrt(d_k)`

As the dimension `d_k` increases, the magnitude of dot products increases, which makes the softmax **too peaky**, leading to poor gradients.

To stabilize this, we **scale** the scores:
```
scaled_scores = attention_scores / sqrt(d_k)
```

This prevents extreme softmax outputs and encourages smoother attention distributions.

---

## üéØ 5. Calculating Attention Weights: `softmax(scores, dim=-1)`

To convert raw scores into **probability-like weights**, we apply softmax along each row:

```
attention_weights = softmax(scaled_scores, dim=-1)
```

Each row now sums to 1, and represents:
> How much a given token (as a query) attends to every other token (as keys)

The softmax emphasizes the **most relevant** tokens while preserving differentiability.

---

## üßæ 6. Generating the Context Vector: `attention_weights @ V`

The final **context vector** for each token is calculated as:

```
context_vectors = attention_weights @ V
```

This computes a **weighted sum of value vectors**, where weights come from the attention mechanism.

### Interpretation:
- Each token‚Äôs context vector is a **mixture** of all other token values.
- Tokens it attends to more (higher weights) contribute more to its final vector.
- This allows the model to **"rephrase"** the token‚Äôs meaning using the whole sentence.

---

## ‚úÖ Summary: Full Attention Pipeline

```text
Input Embedding Matrix (X)
        ‚Üì
Q = X @ Wq       ‚Üí What I'm looking for
K = X @ Wk       ‚Üí What I contain
V = X @ Wv       ‚Üí What I can offer
        ‚Üì
Dot Product: att_scores = Q @ K·µÄ
Scale:       att_scores_scaled = att_scores / sqrt(d_k)
Softmax:     att_weights = softmax(att_scores_scaled)
Weighted Sum: context_vectors = att_weights @ V
```

---

## üìå Final Intuition: Why Is the Context Vector Powerful?

- **Retains original meaning** + adds **rich contextual info**
- Differentiates usage of the same word in different sentences (e.g., "bank of river" vs "bank loan")
- Allows **long-range dependencies** to be captured easily (no sequential bottleneck)

> Each context vector is like a **rewritten version of a word**, rewritten with awareness of its surroundings.

---


Here's a full explanation of the **context vector** and the **attention mechanism**, starting from **trainable matrices** to **why we use softmax and scaling**, and ending with how it gives us **context-enriched embeddings**.

---

## üìå 1. What is a **Context Vector**?

The **context vector** is the output of the attention mechanism for a token ‚Äî it's a new embedding that contains:
- Its **own meaning** (from its original embedding)
- Plus **related information from other tokens** in the sequence (via weighted combinations)

In short:
> **Context Vector = Enriched Embedding**
> (It embeds not just the token itself, but also what it's "paying attention to" in its context)

---

## ‚öôÔ∏è 2. Trainable Weight Matrices ‚Äî `Wq`, `Wk`, `Wv`

When a token is passed into a transformer, it gets converted into **three different roles** using **three separate trainable weight matrices**:

| Name | Role | Transformation | Dim |
|------|------|----------------|-----|
| **W<sub>q</sub>** | Query | captures *"what am I looking for?"* | [d<sub>model</sub> √ó d<sub>k</sub>] |
| **W<sub>k</sub>** | Key | captures *"what do I contain?"* | [d<sub>model</sub> √ó d<sub>k</sub>] |
| **W<sub>v</sub>** | Value | carries content to deliver | [d<sub>model</sub> √ó d<sub>v</sub>] |

Applied as:

```python
Q = X @ Wq
K = X @ Wk
V = X @ Wv
```

Where:
- `X`: input token embeddings (e.g. from word2vec or learned embeddings)
- Resulting `Q`, `K`, and `V` are used to calculate attention.

---

## üîç 3. Why `att_scores = Q @ K·µÄ`?

The attention score tells **how relevant one token is to another**.

This is calculated by:
```python
att_scores = Q @ K.T
```

Reason:
- It‚Äôs a **dot product similarity** ‚Äî measures alignment between queries and keys.
- High score: query and key are semantically aligned.
- `Q[i] @ K[j]` tells how much token `i` should focus on token `j`.

This gives a **(n_tokens √ó n_tokens)** matrix.

---

## üìê 4. Why do we scale by `sqrt(d)`?

If the **dimension d<sub>k</sub> is large**, then dot product values can become large.

Effect:
- Large values ‚Üí softmax becomes spiky ‚Üí poor gradient flow during training.

To fix this:
```python
att_scores_scaled = att_scores / sqrt(d_k)
```

‚úÖ This keeps the distribution of attention values **stable**, allowing softmax to work well.

---

## üßÆ 5. Why `softmax(att_scores_scaled, dim=-1)`?

Softmax converts raw scores into **normalized attention weights**:

```python
att_weights = softmax(att_scores_scaled, dim=-1)
```

- Output: matrix of probabilities, each row sums to 1.
- `att_weights[i][j]`: how much token `i` attends to token `j`.

Think of it like **"how much I care about you"** between every token pair.

---

## üéØ 6. Finally: `Context = att_weights @ V`

Once you have the attention weights, you do a **weighted sum of values**:
```python
context_vectors = att_weights @ V
```

- Each row in `context_vectors` represents the **final output vector** for a token.
- It's enriched with information from the whole sequence.

üí° This is where the **magic happens** ‚Äî each token‚Äôs output is **a summary of the sentence from its perspective**.

---

## ‚úÖ Recap Summary

| Step | Formula | Meaning |
|------|---------|---------|
| Project | `Q = X @ Wq`, `K = X @ Wk`, `V = X @ Wv` | Encode queries, keys, values |
| Score | `att_scores = Q @ K.T` | Dot product similarity |
| Scale | `att_scores / sqrt(d_k)` | Normalize for stability |
| Weights | `att_weights = softmax(...)` | Normalize focus per token |
| Output | `context = att_weights @ V` | Context-aware embeddings |

---

Would you like this visualized as a step-by-step diagram or animation too?
