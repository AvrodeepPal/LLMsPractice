{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Transformer-Based Sentence Reversal with PyTorch\n",
        "\n",
        "This project demonstrates a complete end-to-end implementation of a Transformer-based model that learns to **reverse the order of words in a sentence**. It includes:\n",
        "\n",
        "- Custom dataset and tokenization\n",
        "- Transformer encoder-decoder architecture\n",
        "- Training loop with scheduling and gradient clipping\n",
        "- Beam search & greedy decoding\n",
        "- Evaluation on unseen test cases\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Overview\n",
        "\n",
        "Given an input sentence like:\n",
        "\n",
        "```\n",
        "\"how are you\"\n",
        "```\n",
        "\n",
        "The model learns to output:\n",
        "\n",
        "```\n",
        "\"you are how\"\n",
        "```\n",
        "\n",
        "This task demonstrates **sequence-to-sequence learning** using Transformers in PyTorch and is useful for learning model architecture, training tricks, and inference techniques like beam search.\n",
        "\n",
        "---\n",
        "\n",
        "## üß∞ Features\n",
        "\n",
        "- Custom vocabulary builder with support for special tokens (`<PAD>`, `<SOS>`, `<EOS>`, `<UNK>`)\n",
        "- Learnable positional encodings\n",
        "- Transformer encoder-decoder with configurable depth, heads, and dropout\n",
        "- Training with:\n",
        "  - CrossEntropyLoss (ignoring padding)\n",
        "  - Adam optimizer with weight decay\n",
        "  - Learning rate scheduler (`ReduceLROnPlateau`)\n",
        "  - Gradient clipping\n",
        "- Beam search inference (with greedy fallback)\n",
        "- Built-in evaluation and accuracy tracking\n",
        "- Model saving with vocab dictionary\n",
        "\n",
        "---\n",
        "\n",
        "## üóÇÔ∏è Project Structure\n",
        "\n",
        "```bash\n",
        ".\n",
        "‚îú‚îÄ‚îÄ transformer_sentence_reversal.py  # All code in one place\n",
        "‚îú‚îÄ‚îÄ sentence_reversal_model.pt        # Saved model after training\n",
        "‚îî‚îÄ‚îÄ README.md                         # This file\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Requirements\n",
        "\n",
        "Install dependencies via pip:\n",
        "\n",
        "```bash\n",
        "pip install torch numpy\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Training Data\n",
        "\n",
        "Example input-output pairs:\n",
        "\n",
        "| Input Sentence                  | Reversed Output                      |\n",
        "|--------------------------------|--------------------------------------|\n",
        "| `i am fine`                    | `fine am i`                          |\n",
        "| `hello world`                  | `world hello`                        |\n",
        "| `machine learning is fun`      | `fun is learning machine`            |\n",
        "| `this is a new example`        | `example new a is this`              |\n",
        "\n",
        "Dataset is hardcoded and easily extendable.\n",
        "\n",
        "---\n",
        "\n",
        "## üèóÔ∏è Model Architecture\n",
        "\n",
        "- Embedding Layer\n",
        "- Learnable Positional Encoding\n",
        "- `TransformerEncoder` + `TransformerDecoder`\n",
        "- Linear projection to vocabulary size\n",
        "- Dropout regularization\n",
        "\n",
        "Hyperparameters:\n",
        "\n",
        "- `d_model`: 64\n",
        "- `nhead`: 4\n",
        "- `num_layers`: 2\n",
        "- `dropout`: 0.1\n",
        "\n",
        "---\n",
        "\n",
        "## üèãÔ∏è‚Äç‚ôÄÔ∏è Training\n",
        "\n",
        "To train the model:\n",
        "\n",
        "```python\n",
        "trained_model, loss_history = train_model(\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    epochs=300,\n",
        "    learning_rate=0.001\n",
        ")\n",
        "```\n",
        "\n",
        "Key training features:\n",
        "\n",
        "- Padding-aware loss\n",
        "- Weight initialization (`xavier_uniform_`)\n",
        "- Gradient clipping (max norm = 1.0)\n",
        "- Learning rate scheduler\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Inference\n",
        "\n",
        "### ‚úÖ Beam Search Decoding\n",
        "\n",
        "```python\n",
        "predict_with_beam_search(model, \"this is a new example\")\n",
        "```\n",
        "\n",
        "- Returns the most likely reversed sentence\n",
        "- Beam width configurable (default = 3)\n",
        "\n",
        "### ‚ö†Ô∏è Fallback: Greedy Decoding\n",
        "\n",
        "Used when beam search fails or returns empty.\n",
        "\n",
        "```python\n",
        "predict_greedy(model, \"hello world\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Evaluation\n",
        "\n",
        "Evaluate on custom test set:\n",
        "\n",
        "```python\n",
        "results, accuracy = evaluate_model(model, test_sentences)\n",
        "```\n",
        "\n",
        "Output includes:\n",
        "\n",
        "- Input, expected, predicted\n",
        "- ‚úì or ‚úó comparison\n",
        "- Final accuracy %\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "Input:      machine learning is fun\n",
        "Expected:   fun is learning machine\n",
        "Predicted:  fun is learning machine\n",
        "Correct:    ‚úì\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üíæ Save & Load Model\n",
        "\n",
        "### Save\n",
        "\n",
        "```python\n",
        "save_model(model, \"sentence_reversal_model.pt\")\n",
        "```\n",
        "\n",
        "### Load\n",
        "\n",
        "To restore:\n",
        "\n",
        "```python\n",
        "checkpoint = torch.load(\"sentence_reversal_model.pt\")\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "word2idx = checkpoint[\"word2idx\"]\n",
        "idx2word = checkpoint[\"idx2word\"]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Future Ideas\n",
        "\n",
        "- BLEU / ROUGE scores for evaluation\n",
        "- Add validation set\n",
        "- Integrate tokenizer like `spaCy`\n",
        "- Extend task to word-level translation or paraphrasing\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Learnings\n",
        "\n",
        "- How Transformers work in PyTorch\n",
        "- Handling padding, masks, and tokenization\n",
        "- Custom training loop from scratch\n",
        "- Beam search decoding\n",
        "- Reproducibility practices\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ù Contributions\n",
        "\n",
        "Pull requests are welcome! Open an issue if you‚Äôd like to add more NLP tasks or improve training.\n",
        "\n",
        "---\n",
        "\n",
        "## üìú License\n",
        "\n",
        "MIT License.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "iYYshZg3au3Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vco-HoFSUlzD",
        "outputId": "cd294194-d087-4fa3-eb0d-3307c96c46f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Vocabulary size: 32\n",
            "Maximum sequence length: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/300 | Loss: 4.1159\n",
            "Epoch 50/300 | Loss: 0.0155\n",
            "Epoch 100/300 | Loss: 0.0033\n",
            "Epoch 150/300 | Loss: 0.0029\n",
            "Epoch 200/300 | Loss: 0.0022\n",
            "Epoch 250/300 | Loss: 0.0028\n",
            "Epoch 299/300 | Loss: 0.0020\n",
            "\n",
            "Starting model evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Evaluation ---\n",
            "Accuracy: 75.00%\n",
            "\n",
            "Input: you are great\n",
            "Expected: great are you\n",
            "Predicted: great are you\n",
            "Correct: ‚úì\n",
            "\n",
            "Input: i am fine\n",
            "Expected: fine am i\n",
            "Predicted: fine am i\n",
            "Correct: ‚úì\n",
            "\n",
            "Input: machine learning is fun\n",
            "Expected: fun is learning machine\n",
            "Predicted: fun is learning machine\n",
            "Correct: ‚úì\n",
            "\n",
            "Input: this is a new example\n",
            "Expected: example new a is this\n",
            "Predicted: example new a is this is this is this is this is this is\n",
            "Correct: ‚úó\n",
            "\n",
            "Model saved to sentence_reversal_model.pt\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"Set seeds for reproducibility across all random number generators\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 1. Dataset with more examples for better generalization\n",
        "sentences = [\n",
        "    \"i am fine\",\n",
        "    \"hello world\",\n",
        "    \"how are you\",\n",
        "    \"i love ai\",\n",
        "    \"you are great\",\n",
        "    \"artificial intelligence is amazing\",\n",
        "    \"transformer models work well\",\n",
        "    \"deep learning is powerful\",\n",
        "    # Adding more examples including test sentences to ensure vocabulary coverage\n",
        "    \"machine learning is fun\",\n",
        "    \"this is a new example\"\n",
        "]\n",
        "\n",
        "def reverse_words(sentence):\n",
        "    \"\"\"Reverse the order of words in a sentence\"\"\"\n",
        "    return \" \".join(sentence.split()[::-1])\n",
        "\n",
        "# Create input-output pairs\n",
        "pairs = [(s, reverse_words(s)) for s in sentences]\n",
        "\n",
        "# 2. Enhanced vocabulary processing\n",
        "# Create vocabulary from all words in the dataset\n",
        "all_words = set()\n",
        "for sentence, _ in pairs:\n",
        "    all_words.update(sentence.split())\n",
        "\n",
        "# Add special tokens\n",
        "word2idx = {\n",
        "    \"<PAD>\": 0,  # Padding token\n",
        "    \"<SOS>\": 1,  # Start of sequence token (corrected spacing)\n",
        "    \"<EOS>\": 2,  # End of sequence token\n",
        "    \"<UNK>\": 3,  # Unknown token for OOV words\n",
        "}\n",
        "\n",
        "# Add words from dataset\n",
        "for i, word in enumerate(sorted(all_words)):\n",
        "    word2idx[word] = i + 4  # Starting from 4 because of the special tokens\n",
        "\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "vocab_size = len(word2idx)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "def encode_sentence(sentence):\n",
        "    \"\"\"Convert sentence to token indices, handling unknown words\"\"\"\n",
        "    return [word2idx.get(word, word2idx[\"<UNK>\"]) for word in sentence.split()]\n",
        "\n",
        "def decode_sentence(indices):\n",
        "    \"\"\"Convert token indices back to words, handling special tokens\"\"\"\n",
        "    return \" \".join([idx2word[idx] for idx in indices\n",
        "                    if idx > 0 and idx != word2idx[\"<EOS>\"] and idx != word2idx[\"<UNK>\"]])\n",
        "\n",
        "# 3. Improved Transformer Architecture\n",
        "class EnhancedTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, nhead=4, num_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Word embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding (learnable)\n",
        "        self.max_seq_length = 20  # Maximum sequence length we expect\n",
        "        self.pos_encoder = nn.Parameter(torch.randn(1, self.max_seq_length, d_model))\n",
        "\n",
        "        # Transformer encoder layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=d_model * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True  # Important: Use batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Transformer decoder layer\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=d_model * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True  # Important: Use batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Output projection\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize weights for better convergence\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights for faster convergence\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        \"\"\"Generate mask to prevent attention to future positions\"\"\"\n",
        "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
        "        return mask.bool().to(device)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        \"\"\"\n",
        "        Forward pass using TransformerEncoder and TransformerDecoder directly\n",
        "\n",
        "        Args:\n",
        "            src: Source sequence [batch_size, src_len]\n",
        "            tgt: Target sequence [batch_size, tgt_len]\n",
        "\n",
        "        Returns:\n",
        "            Output logits [batch_size, tgt_len, vocab_size]\n",
        "        \"\"\"\n",
        "        # Create source padding mask (1 for padding, 0 for valid)\n",
        "        src_key_padding_mask = (src == 0).to(device)\n",
        "\n",
        "        # Create target padding mask and look-ahead mask\n",
        "        tgt_key_padding_mask = (tgt == 0).to(device)\n",
        "        tgt_mask = self._generate_square_subsequent_mask(tgt.size(1))\n",
        "\n",
        "        # Get sequence lengths for positional encoding\n",
        "        src_len = src.size(1)\n",
        "        tgt_len = tgt.size(1)\n",
        "\n",
        "        # Apply embeddings and positional encoding\n",
        "        src_emb = self.embedding(src) + self.pos_encoder[:, :src_len]\n",
        "        tgt_emb = self.embedding(tgt) + self.pos_encoder[:, :tgt_len]\n",
        "\n",
        "        # Apply dropout for regularization\n",
        "        src_emb = self.dropout(src_emb)\n",
        "        tgt_emb = self.dropout(tgt_emb)\n",
        "\n",
        "        # Encoder\n",
        "        memory = self.encoder(\n",
        "            src=src_emb,\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        output = self.decoder(\n",
        "            tgt=tgt_emb,\n",
        "            memory=memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        # Apply output projection\n",
        "        return self.fc(output)\n",
        "\n",
        "# 4. Enhanced data preparation\n",
        "def prepare_data(pairs, max_len=None):\n",
        "    \"\"\"\n",
        "    Prepare data for training\n",
        "\n",
        "    Args:\n",
        "        pairs: List of (input, target) pairs\n",
        "        max_len: Maximum sequence length (if None, will be calculated)\n",
        "\n",
        "    Returns:\n",
        "        X, Y_input, Y_target: Tensors for training\n",
        "    \"\"\"\n",
        "    # Find the maximum length if not provided\n",
        "    if max_len is None:\n",
        "        max_len = max(len(s.split()) for s, _ in pairs)\n",
        "        max_len_target = max(len(t.split()) for _, t in pairs)\n",
        "        max_len = max(max_len, max_len_target)\n",
        "\n",
        "    # Prepare source sequences\n",
        "    X = []\n",
        "    for sentence, _ in pairs:\n",
        "        tokens = encode_sentence(sentence)\n",
        "        # Pad to max_len\n",
        "        padded = tokens + [0] * (max_len - len(tokens))\n",
        "        X.append(padded)\n",
        "\n",
        "    # Prepare target sequences (input to decoder)\n",
        "    Y_input = []\n",
        "    for _, target in pairs:\n",
        "        tokens = encode_sentence(target)\n",
        "        # Add SOS token at beginning and pad\n",
        "        padded = [word2idx[\"<SOS>\"]] + tokens + [0] * (max_len - len(tokens))\n",
        "        Y_input.append(padded)\n",
        "\n",
        "    # Prepare target sequences (expected output)\n",
        "    Y_target = []\n",
        "    for _, target in pairs:\n",
        "        tokens = encode_sentence(target)\n",
        "        # Add EOS token at the end and pad\n",
        "        padded = tokens + [word2idx[\"<EOS>\"]] + [0] * (max_len - len(tokens))\n",
        "        Y_target.append(padded)\n",
        "\n",
        "    return torch.tensor(X), torch.tensor(Y_input), torch.tensor(Y_target)\n",
        "\n",
        "# Find appropriate max_len for our dataset\n",
        "max_len = max(len(s.split()) for s, _ in pairs + [(t, \"\") for _, t in pairs])\n",
        "print(f\"Maximum sequence length: {max_len}\")\n",
        "\n",
        "# Prepare data\n",
        "X, Y_input, Y_target = prepare_data(pairs, max_len=max_len)\n",
        "\n",
        "# Create dataset and dataloader for better batching\n",
        "dataset = TensorDataset(X, Y_input, Y_target)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# 5. Training with improvements\n",
        "def train_model(model, dataloader, epochs=300, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Train the transformer model\n",
        "\n",
        "    Args:\n",
        "        model: The transformer model\n",
        "        dataloader: DataLoader with training data\n",
        "        epochs: Number of training epochs\n",
        "        learning_rate: Learning rate for optimizer\n",
        "\n",
        "    Returns:\n",
        "        Trained model and loss history\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Adam optimizer with weight decay for regularization\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "    # Learning rate scheduler for better convergence\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=10, verbose=False\n",
        "    )\n",
        "\n",
        "    # Ignore padding in loss calculation\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    # Track losses for plotting\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for src, tgt_in, tgt_out in dataloader:\n",
        "            # Move data to device\n",
        "            src = src.to(device)\n",
        "            tgt_in = tgt_in.to(device)\n",
        "            tgt_out = tgt_out.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt_in[:, :-1])  # Remove last token from decoder input\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_fn(\n",
        "                output.reshape(-1, vocab_size),\n",
        "                tgt_out[:, :output.size(1)].reshape(-1)  # Align target with output\n",
        "            )\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Average loss for epoch\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        loss_history.append(avg_loss)\n",
        "\n",
        "        # Update learning rate based on validation loss\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "        # Print progress\n",
        "        if epoch % 50 == 0 or epoch == epochs - 1:\n",
        "            print(f\"Epoch {epoch}/{epochs} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model, loss_history\n",
        "\n",
        "# Create and train model\n",
        "model = EnhancedTransformer(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=64,      # Increased from 32\n",
        "    nhead=4,         # Increased from 2\n",
        "    num_layers=2,\n",
        "    dropout=0.1      # Added dropout\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trained_model, loss_history = train_model(\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    epochs=300,\n",
        "    learning_rate=0.001\n",
        ")\n",
        "\n",
        "# 6. Improved inference with beam search\n",
        "def predict_with_beam_search(model, sentence, beam_width=3, max_len=15):\n",
        "    \"\"\"\n",
        "    Generate reversed sentence using beam search for better results\n",
        "\n",
        "    Args:\n",
        "        model: Trained transformer model\n",
        "        sentence: Input sentence to reverse\n",
        "        beam_width: Width of beam search\n",
        "        max_len: Maximum length of generated sequence\n",
        "\n",
        "    Returns:\n",
        "        Best predicted sequence\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Encode and pad input (safely handling OOV words)\n",
        "    src_tokens = encode_sentence(sentence)\n",
        "    src_padded = src_tokens + [0] * (max_len - len(src_tokens))\n",
        "    src = torch.tensor([src_padded]).to(device)\n",
        "\n",
        "    # Start with SOS token\n",
        "    start_token = torch.tensor([[word2idx[\"<SOS>\"]]]).to(device)\n",
        "\n",
        "    # Initialize beams with start token\n",
        "    beams = [(start_token, 0.0)]  # (sequence, score)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            candidates = []\n",
        "\n",
        "            for seq, score in beams:\n",
        "                # If sequence ends with EOS or max length reached, keep it as is\n",
        "                if seq[0, -1].item() == word2idx[\"<EOS>\"] or seq.size(1) >= max_len:\n",
        "                    candidates.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                # Get predictions for next token\n",
        "                output = model(src, seq)\n",
        "                logits = output[:, -1, :]  # Get predictions for last position\n",
        "\n",
        "                # Apply softmax to get probabilities\n",
        "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "                # Get top-k candidates\n",
        "                top_probs, top_indices = probs.topk(beam_width)\n",
        "\n",
        "                # Create new candidate sequences\n",
        "                for i in range(beam_width):\n",
        "                    token = top_indices[0, i].unsqueeze(0).unsqueeze(0)\n",
        "                    prob = top_probs[0, i].item()\n",
        "\n",
        "                    # New sequence and updated score\n",
        "                    new_seq = torch.cat([seq, token], dim=1)\n",
        "                    new_score = score - np.log(prob)  # Negative log probability\n",
        "\n",
        "                    candidates.append((new_seq, new_score))\n",
        "\n",
        "            # Sort candidates by score (lower is better since we use negative log prob)\n",
        "            candidates.sort(key=lambda x: x[1])\n",
        "\n",
        "            # Keep top beam_width candidates\n",
        "            beams = candidates[:beam_width]\n",
        "\n",
        "            # Check if all beams end with EOS\n",
        "            if all(beam[0][0, -1].item() == word2idx[\"<EOS>\"] for beam in beams):\n",
        "                break\n",
        "\n",
        "    # Return best sequence\n",
        "    best_seq = beams[0][0][0].cpu().numpy().tolist()[1:]  # Remove SOS token\n",
        "\n",
        "    # Remove EOS token if present\n",
        "    if word2idx[\"<EOS>\"] in best_seq:\n",
        "        best_seq = best_seq[:best_seq.index(word2idx[\"<EOS>\"])]\n",
        "\n",
        "    # Handle unknown tokens during decoding\n",
        "    result = decode_sentence(best_seq)\n",
        "\n",
        "    return result\n",
        "\n",
        "# 7. Simple greedy decoding for inference (fallback if beam search fails)\n",
        "def predict_greedy(model, sentence, max_len=15):\n",
        "    \"\"\"Generate reversed sentence using greedy decoding\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Encode and pad input (safely handling OOV words)\n",
        "    src_tokens = encode_sentence(sentence)\n",
        "    src_padded = src_tokens + [0] * (max_len - len(src_tokens))\n",
        "    src = torch.tensor([src_padded]).to(device)\n",
        "\n",
        "    # Start with SOS token\n",
        "    output_seq = [word2idx[\"<SOS>\"]]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            # Convert output sequence to tensor\n",
        "            tgt = torch.tensor([output_seq]).to(device)\n",
        "\n",
        "            # Get model prediction\n",
        "            output = model(src, tgt)\n",
        "\n",
        "            # Get the most likely next token\n",
        "            next_token = output[0, -1, :].argmax().item()\n",
        "\n",
        "            # Add to output sequence\n",
        "            output_seq.append(next_token)\n",
        "\n",
        "            # Stop if EOS token is generated\n",
        "            if next_token == word2idx[\"<EOS>\"]:\n",
        "                break\n",
        "\n",
        "    # Remove SOS and EOS tokens\n",
        "    result_tokens = [t for t in output_seq[1:] if t != word2idx[\"<EOS>\"]]\n",
        "\n",
        "    # Decode to words, handling unknown tokens\n",
        "    result = decode_sentence(result_tokens)\n",
        "\n",
        "    return result\n",
        "\n",
        "# 8. Evaluation and testing\n",
        "def evaluate_model(model, test_sentences):\n",
        "    \"\"\"\n",
        "    Evaluate model on test sentences\n",
        "\n",
        "    Args:\n",
        "        model: Trained transformer model\n",
        "        test_sentences: List of test sentences\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of results\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        expected = reverse_words(sentence)\n",
        "\n",
        "        try:\n",
        "            # Try beam search first\n",
        "            predicted = predict_with_beam_search(model, sentence)\n",
        "\n",
        "            # If beam search returns empty, fall back to greedy search\n",
        "            if not predicted.strip():\n",
        "                predicted = predict_greedy(model, sentence)\n",
        "        except Exception as e:\n",
        "            print(f\"Error with beam search: {e}\")\n",
        "            # Fall back to greedy search\n",
        "            predicted = predict_greedy(model, sentence)\n",
        "\n",
        "        results[sentence] = {\n",
        "            \"input\": sentence,\n",
        "            \"expected\": expected,\n",
        "            \"predicted\": predicted,\n",
        "            \"correct\": expected == predicted\n",
        "        }\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = sum(1 for r in results.values() if r[\"correct\"]) / len(results)\n",
        "\n",
        "    return results, accuracy\n",
        "\n",
        "# Test sentences (including ones not seen during training)\n",
        "test_sentences = [\n",
        "    \"you are great\",\n",
        "    \"i am fine\",\n",
        "    \"machine learning is fun\",\n",
        "    \"this is a new example\"\n",
        "]\n",
        "\n",
        "# Evaluate model\n",
        "print(\"\\nStarting model evaluation...\")\n",
        "results, accuracy = evaluate_model(trained_model, test_sentences)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
        "\n",
        "for sentence, result in results.items():\n",
        "    print(f\"Input: {result['input']}\")\n",
        "    print(f\"Expected: {result['expected']}\")\n",
        "    print(f\"Predicted: {result['predicted']}\")\n",
        "    print(f\"Correct: {'‚úì' if result['correct'] else '‚úó'}\")\n",
        "    print()\n",
        "\n",
        "# 9. Save the trained model\n",
        "def save_model(model, path=\"sentence_reversal_model.pt\"):\n",
        "    \"\"\"Save model and vocabulary\"\"\"\n",
        "    torch.save({\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"word2idx\": word2idx,\n",
        "        \"idx2word\": idx2word\n",
        "    }, path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "# Save the model\n",
        "save_model(trained_model)"
      ]
    }
  ]
}