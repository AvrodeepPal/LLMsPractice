# ğŸ§  TinyLLMs to Transformers â€” My Custom LLM Roadmap

Welcome to my personal LLM practice repository, where Iâ€™m documenting my journey through building, training, and experimenting with increasingly complex models â€” from scratch-built tokenizers to LSTM-based stock prediction systems. This repo reflects both my theoretical understanding and hands-on implementations.

---

## ğŸ“ˆ Current Stage: Stage 3 â€“ MemoryLLM (LSTM-based Stock Prediction)

### ğŸš€ Project: `MemoryLLM`
- **Goal**: Predict future stock prices of Google (GOOG) using historical price and technical indicators.
- **Tech Stack**: `Python`, `TensorFlow/Keras`, `pandas`, `matplotlib`
- **Architecture**: LSTM-based deep learning model.
- **Indicators Used**:
  - Moving Averages (SMA/EMA)
  - RSI
  - MACD
  - Bollinger Bands
- **Evaluation Metrics**:
  - RMSE
  - MAE
  - MAPE
  - RÂ² Score
- **Results**:
  - Effective short-term trend prediction
  - Visualization of predictions vs actual values
- **Challenges**:
  - Struggled with long-range dependencies
  - LSTM hidden states are hard to interpret
  - Performance bottleneck due to sequential training

ğŸ“„ See more: [`LLM5 MemoryLLM.md`](./LLM5%20MemoryLLM.md)

---

## ğŸ“š Roadmap Overview

| Stage | Model Type | Project | Description |
|-------|------------|---------|-------------|
| âœ… 1 | Token-level RNN | `TinyLLM` | Basic character prediction using a mini-RNN |
| âœ… 2 | Word-level RNN | `SmaLLM` | Word prediction using word embeddings |
| âœ… 3 | LSTM (Memory) | `MemoryLLM` | Stock prediction using LSTM with indicators |
| â³ 4 | Transformer (Attention) | `CodeLLM` | Planned: Build Transformer-based models |
| â³ 5 | Pretrained Models (LLMs) | `LLMind` | Planned: Explore fine-tuning FinGPT / BERT |
| â³ 6 | Hybrid + RL + Agentic | TBD | Planned: Agent-based LLMs & multi-modal inputs |

---

## ğŸ”œ Next Stage: Stage 4 â€“ CodeLLM (Transformer Encoder-Decoder)

### ğŸ¯ Goals
- Build a **Transformer encoder-decoder** model for time-series forecasting.
- Integrate **positional encoding**, **multi-head attention**, and **feed-forward layers**.
- Replace recurrence with **global attention** for better performance on long sequences.

### ğŸ’¡ Why Transformers?
- Capture long-range dependencies better than LSTM
- Faster training through parallelization
- Easier interpretability via attention maps

### ğŸ› ï¸ Upcoming Projects:
- âœ… Rewriting GOOG stock predictor using a custom Transformer
- ğŸ”„ Comparing LSTM vs Transformer performance (accuracy, speed, interpretability)
- ğŸ§ª Experimenting with [FinGPT](https://github.com/AI4Finance-Foundation/FinGPT), [Informer](https://github.com/zhouhaoyi/Informer2020)

ğŸ“„ See comparison: [`LLM10 Stage3 vs Stage4.md`](./LLM10%20Stage3%20vs%20Stage4.md)

---

## ğŸ§ª Folder Structure

```
â”œâ”€â”€ TinyLLM/            # Stage 1: Character-level RNN
â”œâ”€â”€ SmaLLM/             # Stage 2: Word-level RNN
â”œâ”€â”€ MemoryLLM/          # Stage 3: LSTM-based GOOG price predictor
â”œâ”€â”€ CodeLLM/            # Stage 4: (Planned) Transformer-based forecasting
â”œâ”€â”€ LLM2 RoadMap.md     # My detailed multi-stage learning plan
â”œâ”€â”€ LLM5 MemoryLLM.md   # Write-up on LSTM stock prediction model
â”œâ”€â”€ LLM10 Stage3 vs Stage4.md  # Deep dive comparison: LSTM vs Transformer
```

---

## ğŸ§  Knowledge Highlights

- âœ… **Learned**: Sequential modeling, data preprocessing, LSTM architectures, time-series indicators.
- ğŸ”„ **Exploring**: Attention mechanisms, Transformer math, sequence-to-sequence tasks.
- â­ **Next**: Implement Transformer for GOOG price prediction using PyTorch or TensorFlow.

---

## âœï¸ Author

**[Avrodeep Pal]**

- Machine Learning and NLP Enthusiast
- Exploring end-to-end LLM development from scratch
- Always up for a conversation on Transformers, Agents, or Fintech AI

---

## â­ï¸ Star the Repo if You're Also on a Custom LLM Journey!

```
git clone https://github.com/AvrodeepPal/LLMsPractice.git
cd LLMsPractice
```

Feel free to open issues or discussions if you're following a similar path â€” would love to exchange ideas or collaborate!

---
```

---
