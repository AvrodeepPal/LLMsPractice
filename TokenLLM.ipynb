{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOoMN0u6dlEs3gAzM+SzRG+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "94d0b88fee604c0ca4782b819954434d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a38ecc039db40cfad9e57d9f6b0b790",
              "IPY_MODEL_d351f0016b6d4e999628d18cd8844c6e",
              "IPY_MODEL_f9a0a13caaf6455a9fe01f47d963e0df"
            ],
            "layout": "IPY_MODEL_d98eb2d872f24bc89447c9ff2919ddad"
          }
        },
        "5a38ecc039db40cfad9e57d9f6b0b790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e3677277af94e9dadd930362a7925d7",
            "placeholder": "​",
            "style": "IPY_MODEL_8950d1865b69468799fdc7632d3bf66b",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "d351f0016b6d4e999628d18cd8844c6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5470ad2eb47497e86295c462c1b9fcf",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f25795337a7649a08be59ea3f8fdd83f",
            "value": 48
          }
        },
        "f9a0a13caaf6455a9fe01f47d963e0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff78cd3a4601418ebcace1e70184fea1",
            "placeholder": "​",
            "style": "IPY_MODEL_f23f9e46493b42a8bb28ab3d1cdb1201",
            "value": " 48.0/48.0 [00:00&lt;00:00, 6.00kB/s]"
          }
        },
        "d98eb2d872f24bc89447c9ff2919ddad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e3677277af94e9dadd930362a7925d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8950d1865b69468799fdc7632d3bf66b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5470ad2eb47497e86295c462c1b9fcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f25795337a7649a08be59ea3f8fdd83f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff78cd3a4601418ebcace1e70184fea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f23f9e46493b42a8bb28ab3d1cdb1201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39236743b89144d59c447e21e54cf34c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f0cdf0101764144b28cefa0ee446faf",
              "IPY_MODEL_fef76593175a464591b7ea493bb777a8",
              "IPY_MODEL_872e267c35ce47858991e776a49dd0b5"
            ],
            "layout": "IPY_MODEL_589be0b43fe34e67a18fd3e34f1f7459"
          }
        },
        "9f0cdf0101764144b28cefa0ee446faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2eabfbc9c334f7a91a2b2dcecd3f3ba",
            "placeholder": "​",
            "style": "IPY_MODEL_49663639cdaa4bb59312fe0a45adfc18",
            "value": "config.json: 100%"
          }
        },
        "fef76593175a464591b7ea493bb777a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6e3b9f7ec414a3ca3f319942977cf96",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3695fef337d6408799f9a1a2045c01f4",
            "value": 570
          }
        },
        "872e267c35ce47858991e776a49dd0b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86ffe74e47af4d5e84c2069342876be8",
            "placeholder": "​",
            "style": "IPY_MODEL_4fb421a4e8404845b0311f728695ff51",
            "value": " 570/570 [00:00&lt;00:00, 79.4kB/s]"
          }
        },
        "589be0b43fe34e67a18fd3e34f1f7459": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2eabfbc9c334f7a91a2b2dcecd3f3ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49663639cdaa4bb59312fe0a45adfc18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6e3b9f7ec414a3ca3f319942977cf96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3695fef337d6408799f9a1a2045c01f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86ffe74e47af4d5e84c2069342876be8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fb421a4e8404845b0311f728695ff51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04caedf79ee94e2fbe4a51492393cc0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c07ecf12a53c4ece8fac833ff17250a6",
              "IPY_MODEL_b6caca5c9a0742de9054b333e9351882",
              "IPY_MODEL_6484089b94a0422c98f4458deb55c0bf"
            ],
            "layout": "IPY_MODEL_606aa6443bf74f2290217be44c9293e8"
          }
        },
        "c07ecf12a53c4ece8fac833ff17250a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9f2e8348cb84dbab235878a1db02e6e",
            "placeholder": "​",
            "style": "IPY_MODEL_65f7e0dabeea4b9aa751fd56017cfe88",
            "value": "vocab.txt: 100%"
          }
        },
        "b6caca5c9a0742de9054b333e9351882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a25beee3ffa44c8bbe6ac2de4cb514f8",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea8e6a5995d44fb9969f19233a5555e7",
            "value": 231508
          }
        },
        "6484089b94a0422c98f4458deb55c0bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eaf059fe23146a3916f235afd97f24b",
            "placeholder": "​",
            "style": "IPY_MODEL_1a6d642bac334ffcac9787874074416f",
            "value": " 232k/232k [00:00&lt;00:00, 5.38MB/s]"
          }
        },
        "606aa6443bf74f2290217be44c9293e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9f2e8348cb84dbab235878a1db02e6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65f7e0dabeea4b9aa751fd56017cfe88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a25beee3ffa44c8bbe6ac2de4cb514f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea8e6a5995d44fb9969f19233a5555e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9eaf059fe23146a3916f235afd97f24b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a6d642bac334ffcac9787874074416f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7ee919efb794047bd2d0fdb392a2187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e6ea61d1cba47398243328beb8d8e09",
              "IPY_MODEL_891626f37fa14575ae4985b78681bcf9",
              "IPY_MODEL_fb7c5ddd5b4e4d91825314a8be0bb02a"
            ],
            "layout": "IPY_MODEL_48ed5d7802f740b8ad69799c04c4adf3"
          }
        },
        "5e6ea61d1cba47398243328beb8d8e09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9472d06c31a44b69ce2e43604648d15",
            "placeholder": "​",
            "style": "IPY_MODEL_9c6c2a8cce3846dfbc1f2b595b7b24bd",
            "value": "tokenizer.json: 100%"
          }
        },
        "891626f37fa14575ae4985b78681bcf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb5a9d83613e44d4917fc62a7dc65dc6",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ff07869687448d6a61ead5609dabdff",
            "value": 466062
          }
        },
        "fb7c5ddd5b4e4d91825314a8be0bb02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe3f393c1b334406a12a7e10d23794ea",
            "placeholder": "​",
            "style": "IPY_MODEL_019bffbb816a4402b4cf54f3e16bc380",
            "value": " 466k/466k [00:02&lt;00:00, 193kB/s]"
          }
        },
        "48ed5d7802f740b8ad69799c04c4adf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9472d06c31a44b69ce2e43604648d15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c6c2a8cce3846dfbc1f2b595b7b24bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb5a9d83613e44d4917fc62a7dc65dc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ff07869687448d6a61ead5609dabdff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe3f393c1b334406a12a7e10d23794ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "019bffbb816a4402b4cf54f3e16bc380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvrodeepPal/LLMsPractice/blob/main/TokenLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 LLM Dataset Preprocessor: Gutenberg Edition\n",
        "\n",
        "This script automates the process of downloading classic novels from [Project Gutenberg](https://www.gutenberg.org/), preprocessing the text, tokenizing sentences using a pretrained Hugging Face model, and decoding tokens back into readable text. This is ideal for creating clean datasets for LLM fine-tuning — including LoRA/PEFT workflows.\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Features\n",
        "\n",
        "- 📥 **Downloads** a random novel from Project Gutenberg\n",
        "- ✂️ **Splits** it into clean, tokenized sentences\n",
        "- 🧹 **Cleans** each sentence (lowercasing, punctuation removal, whitespace normalization)\n",
        "- 🔢 **Tokenizes** sentences using `bert-base-uncased`\n",
        "- 🧠 **Decodes** the tokenized IDs back into readable text\n",
        "- ⚡ **Batch processing** and tqdm progress bars\n",
        "- 🔧 Modular code structure for integration with larger pipelines\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ Requirements\n",
        "\n",
        "### Python Packages\n",
        "Install dependencies via:\n",
        "\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "**`requirements.txt`**\n",
        "```txt\n",
        "transformers\n",
        "nltk\n",
        "tqdm\n",
        "requests\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 📂 Output Files\n",
        "\n",
        "| File            | Description                                   |\n",
        "|-----------------|-----------------------------------------------|\n",
        "| `novel.txt`     | One sentence per line from the downloaded novel |\n",
        "| `encoded.txt`   | Tokenized version of each sentence (token IDs) |\n",
        "| `decoded.txt`   | Reconstructed text from token IDs              |\n",
        "\n",
        "---\n",
        "\n",
        "## 📋 Step-by-Step Process\n",
        "\n",
        "### ✅ Step 1: Download & Sentence Splitting\n",
        "- Selects a random classic novel.\n",
        "- Downloads full text.\n",
        "- Removes Gutenberg headers/footers.\n",
        "- Splits the novel into individual sentences using NLTK or regex fallback.\n",
        "- Stores result in `novel.txt`.\n",
        "\n",
        "### ✅ Step 2: Preprocessing & Tokenization\n",
        "- Converts all text to lowercase.\n",
        "- Strips punctuation and special characters (`a-z`, whitespace only).\n",
        "- Tokenizes each sentence using `bert-base-uncased`.\n",
        "- Stores tokenized IDs to `encoded.txt`.\n",
        "\n",
        "### ✅ Step 3: Decoding Tokens to Text\n",
        "- Converts token IDs back into readable English sentences.\n",
        "- Uses the same tokenizer model for consistency.\n",
        "- Stores output to `decoded.txt`.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 Usage\n",
        "\n",
        "Run the full pipeline:\n",
        "```bash\n",
        "python your_script_name.py\n",
        "```\n",
        "\n",
        "You should see output like:\n",
        "```bash\n",
        "Resource punkt is already downloaded.\n",
        "Selected book: Dracula (ID: 345)\n",
        "Successfully processed 4,251 sentences to novel.txt\n",
        "Successfully tokenized all sentences to encoded.txt\n",
        "Successfully decoded all tokens to decoded.txt\n",
        "Total execution time: 32.87 seconds\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ Customization\n",
        "\n",
        "- **Tokenizer**: Replace `\"bert-base-uncased\"` with any Hugging Face tokenizer.\n",
        "- **Book List**: Add more books to the `gutenberg_books` list in `segment_1_download_and_split()`.\n",
        "- **Cleaning**: Adjust regex cleaning rules to preserve punctuation or casing if needed.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Use Case: LoRA / PEFT Training\n",
        "\n",
        "This pipeline is ideal for preparing clean sentence-level data for:\n",
        "- Instruction tuning\n",
        "- Language modeling (next token prediction)\n",
        "- Encoder-decoder training (translation, summarization)\n",
        "- Parameter-efficient fine-tuning using [LoRA](https://arxiv.org/abs/2106.09685) or [PEFT](https://github.com/huggingface/peft)\n",
        "\n",
        "You can further export `encoded.txt` into `.jsonl`, `.parquet`, or `datasets.Dataset` formats.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 Notes\n",
        "\n",
        "- Ensure you have internet access for downloading novels and Hugging Face tokenizers.\n",
        "- Some books may use slightly different formats — the script tries alternative URLs automatically.\n",
        "- Sentence splitting uses NLTK's `sent_tokenize()` and falls back to regex if resources are missing.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 Sample Pipeline Snippet\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "sentence = \"It was the best of times, it was the worst of times.\"\n",
        "cleaned = re.sub(r'[^a-z\\s]', '', sentence.lower())\n",
        "tokens = tokenizer(cleaned)[\"input_ids\"]\n",
        "decoded = tokenizer.decode(tokens)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧵 Coming Soon (Ideas to Extend)\n",
        "\n",
        "- ✅ Parallelization using `concurrent.futures` (for faster batch preprocessing)\n",
        "- ✅ Save intermediate files as `.jsonl` or `datasets.Dataset`\n",
        "- ✅ Add command-line flags (book selection, tokenizer choice, etc.)\n",
        "- ✅ Word-level masking for pretraining\n",
        "- ✅ Named Entity Recognition (NER) tagging\n",
        "\n",
        "---\n",
        "\n",
        "## 📄 License\n",
        "\n",
        "MIT License ©️ 2025 Avrodeep Pal\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UPNH9i7wEUZ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599,
          "referenced_widgets": [
            "94d0b88fee604c0ca4782b819954434d",
            "5a38ecc039db40cfad9e57d9f6b0b790",
            "d351f0016b6d4e999628d18cd8844c6e",
            "f9a0a13caaf6455a9fe01f47d963e0df",
            "d98eb2d872f24bc89447c9ff2919ddad",
            "2e3677277af94e9dadd930362a7925d7",
            "8950d1865b69468799fdc7632d3bf66b",
            "f5470ad2eb47497e86295c462c1b9fcf",
            "f25795337a7649a08be59ea3f8fdd83f",
            "ff78cd3a4601418ebcace1e70184fea1",
            "f23f9e46493b42a8bb28ab3d1cdb1201",
            "39236743b89144d59c447e21e54cf34c",
            "9f0cdf0101764144b28cefa0ee446faf",
            "fef76593175a464591b7ea493bb777a8",
            "872e267c35ce47858991e776a49dd0b5",
            "589be0b43fe34e67a18fd3e34f1f7459",
            "e2eabfbc9c334f7a91a2b2dcecd3f3ba",
            "49663639cdaa4bb59312fe0a45adfc18",
            "b6e3b9f7ec414a3ca3f319942977cf96",
            "3695fef337d6408799f9a1a2045c01f4",
            "86ffe74e47af4d5e84c2069342876be8",
            "4fb421a4e8404845b0311f728695ff51",
            "04caedf79ee94e2fbe4a51492393cc0b",
            "c07ecf12a53c4ece8fac833ff17250a6",
            "b6caca5c9a0742de9054b333e9351882",
            "6484089b94a0422c98f4458deb55c0bf",
            "606aa6443bf74f2290217be44c9293e8",
            "c9f2e8348cb84dbab235878a1db02e6e",
            "65f7e0dabeea4b9aa751fd56017cfe88",
            "a25beee3ffa44c8bbe6ac2de4cb514f8",
            "ea8e6a5995d44fb9969f19233a5555e7",
            "9eaf059fe23146a3916f235afd97f24b",
            "1a6d642bac334ffcac9787874074416f",
            "d7ee919efb794047bd2d0fdb392a2187",
            "5e6ea61d1cba47398243328beb8d8e09",
            "891626f37fa14575ae4985b78681bcf9",
            "fb7c5ddd5b4e4d91825314a8be0bb02a",
            "48ed5d7802f740b8ad69799c04c4adf3",
            "b9472d06c31a44b69ce2e43604648d15",
            "9c6c2a8cce3846dfbc1f2b595b7b24bd",
            "cb5a9d83613e44d4917fc62a7dc65dc6",
            "4ff07869687448d6a61ead5609dabdff",
            "fe3f393c1b334406a12a7e10d23794ea",
            "019bffbb816a4402b4cf54f3e16bc380"
          ]
        },
        "id": "p8qOwUhcyK_p",
        "outputId": "3fb82eeb-cec6-414d-b6f7-c0eb21ede539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resource punkt is already downloaded.\n",
            "Downloading punkt_tab...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEGMENT 1: Downloading and sentence-splitting a novel...\n",
            "Selected book: Alice in Wonderland (ID: 11)\n",
            "Successfully processed 984 sentences to novel.txt\n",
            "\n",
            "SEGMENT 2: Preprocessing and tokenizing sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94d0b88fee604c0ca4782b819954434d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39236743b89144d59c447e21e54cf34c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04caedf79ee94e2fbe4a51492393cc0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7ee919efb794047bd2d0fdb392a2187"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 2990 sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:00<00:00, 43.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully tokenized all sentences to encoded.txt\n",
            "\n",
            "SEGMENT 3: Decoding token IDs back to text...\n",
            "Decoding 2981 tokenized sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:05<00:00,  5.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully decoded all tokens to decoded.txt\n",
            "\n",
            "Total execution time: 12.69 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import re\n",
        "import requests\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Download necessary NLTK data if not already present\n",
        "# Make sure to download both 'punkt' and 'punkt_tab' resources\n",
        "def ensure_nltk_resources():\n",
        "    resources = ['punkt', 'punkt_tab']\n",
        "    for resource in resources:\n",
        "        try:\n",
        "            nltk.data.find(f'tokenizers/{resource}')\n",
        "            print(f\"Resource {resource} is already downloaded.\")\n",
        "        except LookupError:\n",
        "            print(f\"Downloading {resource}...\")\n",
        "            nltk.download(resource)\n",
        "\n",
        "def segment_1_download_and_split():\n",
        "    \"\"\"\n",
        "    Downloads a random novel from Project Gutenberg and splits it into sentences.\n",
        "    \"\"\"\n",
        "    # Some popular Gutenberg books with stable URLs\n",
        "    gutenberg_books = [\n",
        "        {\"id\": 1342, \"title\": \"Pride and Prejudice\"},\n",
        "        {\"id\": 84, \"title\": \"Frankenstein\"},\n",
        "        {\"id\": 11, \"title\": \"Alice in Wonderland\"},\n",
        "        {\"id\": 1661, \"title\": \"The Adventures of Sherlock Holmes\"},\n",
        "        {\"id\": 2701, \"title\": \"Moby Dick\"},\n",
        "        {\"id\": 1400, \"title\": \"Great Expectations\"},\n",
        "        {\"id\": 98, \"title\": \"A Tale of Two Cities\"},\n",
        "        {\"id\": 1232, \"title\": \"The Prince\"},\n",
        "        {\"id\": 74, \"title\": \"The Adventures of Tom Sawyer\"},\n",
        "        {\"id\": 345, \"title\": \"Dracula\"}\n",
        "    ]\n",
        "\n",
        "    # Select a random book\n",
        "    book = random.choice(gutenberg_books)\n",
        "    print(f\"Selected book: {book['title']} (ID: {book['id']})\")\n",
        "\n",
        "    # Construct the URL\n",
        "    url = f\"https://www.gutenberg.org/files/{book['id']}/{book['id']}-0.txt\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        if response.status_code != 200:\n",
        "            # Try alternative URL format\n",
        "            url = f\"https://www.gutenberg.org/cache/epub/{book['id']}/pg{book['id']}.txt\"\n",
        "            response = requests.get(url, timeout=10)\n",
        "\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "        text = response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading the book: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Clean the text by removing Project Gutenberg headers and footers\n",
        "    # This is a simplified approach - may need adjustment for different books\n",
        "    start_marker = \"*** START OF\"\n",
        "    end_marker = \"*** END OF\"\n",
        "\n",
        "    if start_marker in text:\n",
        "        text = text.split(start_marker)[1]\n",
        "    if end_marker in text:\n",
        "        text = text.split(end_marker)[0]\n",
        "\n",
        "    # Alternative approach to sentence tokenization that doesn't require punkt_tab\n",
        "    try:\n",
        "        # Try using regular sent_tokenize first\n",
        "        sentences = sent_tokenize(text)\n",
        "    except LookupError:\n",
        "        # Fallback to a simpler, less accurate sentence splitter\n",
        "        print(\"NLTK sent_tokenize failed. Using simple regex splitter as fallback.\")\n",
        "        # Simple sentence splitter using regex\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "    # Remove empty sentences and those with just whitespace\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    # Write to file, one sentence per line\n",
        "    with open(\"novel.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for sentence in sentences:\n",
        "            f.write(sentence + \"\\n\")\n",
        "\n",
        "    print(f\"Successfully processed {len(sentences)} sentences to novel.txt\")\n",
        "    return True\n",
        "\n",
        "def segment_2_preprocess_and_tokenize():\n",
        "    \"\"\"\n",
        "    Preprocesses each sentence and tokenizes it.\n",
        "    \"\"\"\n",
        "    # Initialize tokenizer (use a smaller/faster model if speed is important)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
        "\n",
        "    # Process sentences in batches\n",
        "    batch_size = 100\n",
        "    sentences = []\n",
        "\n",
        "    try:\n",
        "        with open(\"novel.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            sentences = [line.strip() for line in f if line.strip()]\n",
        "    except FileNotFoundError:\n",
        "        print(\"novel.txt not found! Run segment 1 first.\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Processing {len(sentences)} sentences...\")\n",
        "\n",
        "    with open(\"encoded.txt\", \"w\", encoding=\"utf-8\") as f_out:\n",
        "        # Process in batches with progress bar\n",
        "        for i in tqdm(range(0, len(sentences), batch_size)):\n",
        "            batch = sentences[i:i+batch_size]\n",
        "\n",
        "            # Clean sentences\n",
        "            cleaned_batch = []\n",
        "            for sentence in batch:\n",
        "                # Convert to lowercase\n",
        "                s = sentence.lower()\n",
        "                # Remove everything except a-z and whitespace\n",
        "                s = re.sub(r'[^a-z\\s]', '', s)\n",
        "                # Replace multiple spaces with single space\n",
        "                s = re.sub(r'\\s+', ' ', s).strip()\n",
        "                cleaned_batch.append(s)\n",
        "\n",
        "            # Tokenize batch\n",
        "            token_ids_batch = tokenizer(cleaned_batch, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "            # Write to file\n",
        "            for token_ids in token_ids_batch:\n",
        "                f_out.write(' '.join(map(str, token_ids)) + '\\n')\n",
        "\n",
        "    print(f\"Successfully tokenized all sentences to encoded.txt\")\n",
        "    return True\n",
        "\n",
        "def segment_3_decode():\n",
        "    \"\"\"\n",
        "    Decodes token IDs back to text.\n",
        "    \"\"\"\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
        "\n",
        "    try:\n",
        "        # Read token IDs\n",
        "        with open(\"encoded.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = [line.strip() for line in f if line.strip()]\n",
        "    except FileNotFoundError:\n",
        "        print(\"encoded.txt not found! Run segment 2 first.\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Decoding {len(lines)} tokenized sentences...\")\n",
        "\n",
        "    # Decode and write to file in batches\n",
        "    with open(\"decoded.txt\", \"w\", encoding=\"utf-8\") as f_out:\n",
        "        for i in tqdm(range(0, len(lines), 100)):\n",
        "            batch = lines[i:i+100]\n",
        "\n",
        "            for line in batch:\n",
        "                # Convert space-separated string of IDs to list of integers\n",
        "                try:\n",
        "                    token_ids = list(map(int, line.split()))\n",
        "                    # Decode back to text\n",
        "                    decoded_text = tokenizer.decode(token_ids)\n",
        "                    f_out.write(decoded_text + '\\n')\n",
        "                except ValueError:\n",
        "                    # Skip lines that can't be properly converted\n",
        "                    continue\n",
        "\n",
        "    print(f\"Successfully decoded all tokens to decoded.txt\")\n",
        "    return True\n",
        "\n",
        "def main():\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Ensure all required NLTK resources are available\n",
        "    ensure_nltk_resources()\n",
        "\n",
        "    print(\"SEGMENT 1: Downloading and sentence-splitting a novel...\")\n",
        "    if segment_1_download_and_split():\n",
        "        print(\"\\nSEGMENT 2: Preprocessing and tokenizing sentences...\")\n",
        "        if segment_2_preprocess_and_tokenize():\n",
        "            print(\"\\nSEGMENT 3: Decoding token IDs back to text...\")\n",
        "            segment_3_decode()\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nTotal execution time: {total_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧪 Tokenization Accuracy Evaluation\n",
        "\n",
        "This script evaluates how accurately a tokenized and then decoded text reproduces the original cleaned version. It is useful for checking how much information is preserved after applying a tokenizer (like BERT) and then decoding it back to text.\n",
        "\n",
        "---\n",
        "\n",
        "## 📋 What It Does\n",
        "\n",
        "- **Reads `cleaned.txt`** – the text preprocessed and cleaned before tokenization.\n",
        "- **Reads `decoded.txt`** – the text obtained by decoding the token IDs.\n",
        "- **Cleans both texts** again (lowercasing, removing punctuation).\n",
        "- **Compares** them word by word using `difflib.SequenceMatcher`.\n",
        "- **Calculates**:\n",
        "  - Total words\n",
        "  - Matched words\n",
        "  - Overall tokenization-decoding accuracy (percentage)\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Why Use It?\n",
        "\n",
        "While tokenizers like BERT's are highly effective, they often split or normalize text differently during encoding and decoding (e.g., subwords, special tokens, etc.). This tool helps quantify how \"lossy\" that process is.\n",
        "\n",
        "---\n",
        "\n",
        "## 📁 Files Required\n",
        "\n",
        "- `cleaned.txt`: Cleaned and preprocessed sentences (before tokenization).\n",
        "- `decoded.txt`: Decoded sentences (from token IDs back to text).\n",
        "\n",
        "These files are generated by the earlier stages in your pipeline:\n",
        "- `cleaned.txt`: from `segment_2_preprocess_and_tokenize()`\n",
        "- `decoded.txt`: from `segment_3_decode()`\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 How to Run\n",
        "\n",
        "Make sure you have Python 3 and `tqdm` installed:\n",
        "\n",
        "```bash\n",
        "pip install tqdm\n",
        "```\n",
        "\n",
        "Then run:\n",
        "\n",
        "```bash\n",
        "python token_accuracy.py\n",
        "```\n",
        "\n",
        "You’ll see output like:\n",
        "\n",
        "```\n",
        "Found 10000 original sentences and 10000 decoded sentences\n",
        "100%|████████████████████████████| 10000/10000\n",
        "Tokenization Accuracy: 96.45%\n",
        "Total words: 120000\n",
        "Matched words: 115740\n",
        "```\n",
        "\n",
        "If the accuracy is low, the script provides reasons such as:\n",
        "- Word splitting (e.g., \"cannot\" → \"can not\")\n",
        "- Subword tokenization behavior\n",
        "- Special characters/tokens during decoding\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 Optional Improvements\n",
        "\n",
        "- Switch to character-level similarity metrics\n",
        "- Log differences for manual review\n",
        "- Add support for different tokenizers or models\n",
        "\n",
        "---\n",
        "\n",
        "## 📜 License\n",
        "\n",
        "MIT License — free to use and modify.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Sl637TBIE_--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import difflib\n",
        "from tqdm import tqdm\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean text by converting to lowercase and removing non-alphabetic characters\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def calculate_accuracy():\n",
        "    print(\"Calculating tokenization accuracy...\")\n",
        "\n",
        "    # Read original sentences\n",
        "    try:\n",
        "        # use 'novel' for above code and 'cleaned' for below code\n",
        "        with open(\"cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            original_lines = [line.strip() for line in f if line.strip()]\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: novel.txt not found!\")\n",
        "        return\n",
        "\n",
        "    # Read decoded sentences\n",
        "    try:\n",
        "        with open(\"decoded.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            decoded_lines = [line.strip() for line in f if line.strip()]\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: decoded.txt not found!\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(original_lines)} original sentences and {len(decoded_lines)} decoded sentences\")\n",
        "\n",
        "    # Ensure we have similar lengths to compare\n",
        "    min_length = min(len(original_lines), len(decoded_lines))\n",
        "    if min_length == 0:\n",
        "        print(\"Error: One or both files are empty!\")\n",
        "        return\n",
        "\n",
        "    # Initialize metrics\n",
        "    total_words = 0\n",
        "    matched_words = 0\n",
        "\n",
        "    # Compare each line with sequence matching\n",
        "    for i in tqdm(range(min_length)):\n",
        "        # Preprocess both lines\n",
        "        orig_clean = preprocess_text(original_lines[i])\n",
        "        decoded_clean = preprocess_text(decoded_lines[i])\n",
        "\n",
        "        # Split into words\n",
        "        orig_words = orig_clean.split()\n",
        "        decoded_words = decoded_clean.split()\n",
        "\n",
        "        # Use sequence matcher to find best alignment\n",
        "        matcher = difflib.SequenceMatcher(None, orig_words, decoded_words)\n",
        "\n",
        "        # Count matched words\n",
        "        for match in matcher.get_matching_blocks():\n",
        "            i_start, j_start, match_len = match\n",
        "            if match_len > 0:  # Skip the sentinel at the end\n",
        "                matched_words += match_len\n",
        "\n",
        "        # Add to total word count\n",
        "        total_words += len(orig_words)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    if total_words > 0:\n",
        "        accuracy = (matched_words / total_words) * 100\n",
        "        print(f\"\\nTokenization Accuracy: {accuracy:.2f}%\")\n",
        "        print(f\"Total words: {total_words}\")\n",
        "        print(f\"Matched words: {matched_words}\")\n",
        "\n",
        "        # Detailed breakdown\n",
        "        if accuracy < 95:\n",
        "            print(\"\\nReasons for lower accuracy may include:\")\n",
        "            print(\"- Word splitting differences (e.g., 'cannot' → 'can not')\")\n",
        "            print(\"- Tokenizer normalization (e.g., whitespace handling)\")\n",
        "            print(\"- Special tokens or subword tokenization\")\n",
        "    else:\n",
        "        print(\"No words to compare!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    calculate_accuracy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjB7bOl85PVd",
        "outputId": "9015bc15-737f-4205-f814-08ae6ca1e586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating tokenization accuracy...\n",
            "Found 8803 original sentences and 8803 decoded sentences\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8803/8803 [00:00<00:00, 56395.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenization Accuracy: 100.00%\n",
            "Total words: 74979\n",
            "Matched words: 74979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 Novel Sentence Tokenization & Token ID Pipeline\n",
        "\n",
        "This Python project demonstrates a complete pipeline for:\n",
        "\n",
        "1. Downloading a random novel from [Project Gutenberg](https://www.gutenberg.org/),\n",
        "2. Splitting it into sentences,\n",
        "3. Cleaning and tokenizing the text using a pretrained BERT tokenizer,\n",
        "4. Saving token IDs,\n",
        "5. Decoding token IDs back to text,\n",
        "6. Verifying decoding accuracy.\n",
        "\n",
        "Perfect for tasks like:\n",
        "- Building datasets for language modeling\n",
        "- Experimenting with tokenizers\n",
        "- NLP pipeline demonstrations\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ Features\n",
        "\n",
        "- Downloads 1 out of 10 popular public domain novels\n",
        "- Cleans and splits the book into well-formed sentences\n",
        "- Tokenizes using `bert-base-uncased` (via Hugging Face Transformers)\n",
        "- Parallel processing for fast tokenization & decoding\n",
        "- Cleaned text, token IDs, and decoded text all saved to file\n",
        "- Automatic verification of tokenization–decoding similarity\n",
        "\n",
        "---\n",
        "\n",
        "## 📁 File Outputs\n",
        "\n",
        "| File Name     | Description                                  |\n",
        "|---------------|----------------------------------------------|\n",
        "| `novel.txt`   | Original sentences from the downloaded novel |\n",
        "| `cleaned.txt` | Cleaned, normalized sentences                |\n",
        "| `encoded.txt` | BERT token IDs per sentence (space-separated)|\n",
        "| `decoded.txt` | Decoded text from token IDs                  |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Segments Breakdown\n",
        "\n",
        "### 🔹 Segment 1: Download & Split\n",
        "- Randomly selects a novel from a curated list\n",
        "- Downloads and cleans Project Gutenberg text\n",
        "- Splits into individual sentences using NLTK or regex fallback\n",
        "\n",
        "### 🔹 Segment 2: Preprocess & Tokenize\n",
        "- Cleans each sentence (lowercase, remove punctuation)\n",
        "- Tokenizes using `bert-base-uncased`\n",
        "- Uses `ThreadPoolExecutor` for parallel processing\n",
        "- Saves `cleaned.txt` and `encoded.txt`\n",
        "\n",
        "### 🔹 Segment 3: Decode\n",
        "- Decodes `encoded.txt` (token IDs) back to natural text\n",
        "- Saves to `decoded.txt`\n",
        "- Compares with `cleaned.txt` to verify decoding accuracy\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Getting Started\n",
        "\n",
        "### 1. Clone the repo\n",
        "\n",
        "```bash\n",
        "git clone https://github.com/your-username/novel-tokenization-pipeline.git\n",
        "cd novel-tokenization-pipeline\n",
        "```\n",
        "\n",
        "### 2. Install dependencies\n",
        "\n",
        "```bash\n",
        "pip install transformers nltk tqdm requests\n",
        "```\n",
        "\n",
        "### 3. Run the full pipeline\n",
        "\n",
        "```bash\n",
        "python your_script_name.py\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Example Output\n",
        "\n",
        "```bash\n",
        "Selected book: Frankenstein (ID: 84)\n",
        "Successfully processed 3,542 sentences to novel.txt\n",
        "Processing 3,542 sentences...\n",
        "Successfully tokenized all sentences to encoded.txt and saved cleaned text to cleaned.txt\n",
        "Decoding 3,542 tokenized sentences...\n",
        "Decoding verification: 95.72% similarity with cleaned text\n",
        "Successfully decoded all tokens to decoded.txt\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 Project Goals\n",
        "\n",
        "This project serves as a stepping stone for:\n",
        "\n",
        "- Data preparation for LLM training\n",
        "- Testing tokenizer-decoder fidelity\n",
        "- Building your own text-to-token-to-text pipeline\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 Notes\n",
        "\n",
        "- If `nltk.punkt` is not installed, it will be auto-downloaded.\n",
        "- Regex fallback is used when `sent_tokenize` fails.\n",
        "- You can swap in a different tokenizer/model in Segment 2 easily.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧑‍💻 Author\n",
        "\n",
        "Made with ❤️ by [Avrodeep Pal]\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "lDkViCIKFJSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re\n",
        "import requests\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import concurrent.futures\n",
        "from functools import partial\n",
        "\n",
        "# Download necessary NLTK data if not already present\n",
        "def ensure_nltk_resources():\n",
        "    resources = ['punkt', 'punkt_tab']\n",
        "    for resource in resources:\n",
        "        try:\n",
        "            nltk.data.find(f'tokenizers/{resource}')\n",
        "            print(f\"Resource {resource} is already downloaded.\")\n",
        "        except LookupError:\n",
        "            print(f\"Downloading {resource}...\")\n",
        "            nltk.download(resource)\n",
        "\n",
        "def segment_1_download_and_split():\n",
        "    \"\"\"\n",
        "    Downloads a random novel from Project Gutenberg and splits it into sentences.\n",
        "    \"\"\"\n",
        "    # Some popular Gutenberg books with stable URLs\n",
        "    gutenberg_books = [\n",
        "        {\"id\": 1342, \"title\": \"Pride and Prejudice\"},\n",
        "        {\"id\": 84, \"title\": \"Frankenstein\"},\n",
        "        {\"id\": 11, \"title\": \"Alice in Wonderland\"},\n",
        "        {\"id\": 1661, \"title\": \"The Adventures of Sherlock Holmes\"},\n",
        "        {\"id\": 2701, \"title\": \"Moby Dick\"},\n",
        "        {\"id\": 1400, \"title\": \"Great Expectations\"},\n",
        "        {\"id\": 98, \"title\": \"A Tale of Two Cities\"},\n",
        "        {\"id\": 1232, \"title\": \"The Prince\"},\n",
        "        {\"id\": 74, \"title\": \"The Adventures of Tom Sawyer\"},\n",
        "        {\"id\": 345, \"title\": \"Dracula\"}\n",
        "    ]\n",
        "\n",
        "    # Select a random book\n",
        "    book = random.choice(gutenberg_books)\n",
        "    print(f\"Selected book: {book['title']} (ID: {book['id']})\")\n",
        "\n",
        "    # Construct the URL\n",
        "    url = f\"https://www.gutenberg.org/files/{book['id']}/{book['id']}-0.txt\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        if response.status_code != 200:\n",
        "            # Try alternative URL format\n",
        "            url = f\"https://www.gutenberg.org/cache/epub/{book['id']}/pg{book['id']}.txt\"\n",
        "            response = requests.get(url, timeout=10)\n",
        "\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "        text = response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading the book: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Clean the text by removing Project Gutenberg headers and footers\n",
        "    start_marker = \"*** START OF\"\n",
        "    end_marker = \"*** END OF\"\n",
        "\n",
        "    if start_marker in text:\n",
        "        text = text.split(start_marker)[1]\n",
        "    if end_marker in text:\n",
        "        text = text.split(end_marker)[0]\n",
        "\n",
        "    # Alternative approach to sentence tokenization that doesn't require punkt_tab\n",
        "    try:\n",
        "        # Try using regular sent_tokenize first\n",
        "        sentences = sent_tokenize(text)\n",
        "    except LookupError:\n",
        "        # Fallback to a simpler, less accurate sentence splitter\n",
        "        print(\"NLTK sent_tokenize failed. Using simple regex splitter as fallback.\")\n",
        "        # Simple sentence splitter using regex\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "    # Remove empty sentences and those with just whitespace\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    # Write to file, one sentence per line\n",
        "    with open(\"novel.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for sentence in sentences:\n",
        "            f.write(sentence + \"\\n\")\n",
        "\n",
        "    print(f\"Successfully processed {len(sentences)} sentences to novel.txt\")\n",
        "    return True\n",
        "\n",
        "def clean_sentence(sentence):\n",
        "    \"\"\"Helper function to clean individual sentences\"\"\"\n",
        "    # Convert to lowercase\n",
        "    s = sentence.lower()\n",
        "    # Remove everything except a-z and whitespace\n",
        "    s = re.sub(r'[^a-z\\s]', '', s)\n",
        "    # Replace multiple spaces with single space\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def process_batch(batch, tokenizer):\n",
        "    \"\"\"Process a batch of sentences with tokenizer\"\"\"\n",
        "    # Clean sentences\n",
        "    cleaned_batch = [clean_sentence(sentence) for sentence in batch]\n",
        "\n",
        "    # Tokenize batch\n",
        "    token_ids_batch = tokenizer(cleaned_batch, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "    # Return results\n",
        "    return list(zip(cleaned_batch, [' '.join(map(str, token_ids)) for token_ids in token_ids_batch]))\n",
        "\n",
        "def segment_2_preprocess_and_tokenize():\n",
        "    \"\"\"\n",
        "    Preprocesses each sentence and tokenizes it using parallel processing.\n",
        "    \"\"\"\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
        "\n",
        "    # Read sentences\n",
        "    try:\n",
        "        with open(\"novel.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            sentences = [line.strip() for line in f if line.strip()]\n",
        "    except FileNotFoundError:\n",
        "        print(\"novel.txt not found! Run segment 1 first.\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Processing {len(sentences)} sentences...\")\n",
        "\n",
        "    # Determine optimal batch size and number of workers\n",
        "    batch_size = 100\n",
        "    max_workers = min(32, (len(sentences) // batch_size) + 1)\n",
        "    batches = [sentences[i:i+batch_size] for i in range(0, len(sentences), batch_size)]\n",
        "\n",
        "    # Process batches in parallel\n",
        "    results = []\n",
        "    process_func = partial(process_batch, tokenizer=tokenizer)\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit all batches to the executor\n",
        "        future_to_batch = {executor.submit(process_func, batch): i for i, batch in enumerate(batches)}\n",
        "\n",
        "        # Process results as they complete\n",
        "        for future in tqdm(concurrent.futures.as_completed(future_to_batch), total=len(batches)):\n",
        "            batch_results = future.result()\n",
        "            results.extend(batch_results)\n",
        "\n",
        "    # Sort results based on the order they were submitted\n",
        "    # This is necessary because parallel processing might return results out of order\n",
        "    results.sort(key=lambda x: sentences.index(x[0]) if x[0] in sentences else float('inf'))\n",
        "\n",
        "    # Write results to file - both original cleaned text and tokenized IDs\n",
        "    with open(\"cleaned.txt\", \"w\", encoding=\"utf-8\") as f_cleaned:\n",
        "        with open(\"encoded.txt\", \"w\", encoding=\"utf-8\") as f_encoded:\n",
        "            for cleaned_text, encoded_ids in results:\n",
        "                f_cleaned.write(cleaned_text + '\\n')\n",
        "                f_encoded.write(encoded_ids + '\\n')\n",
        "\n",
        "    print(f\"Successfully tokenized all sentences to encoded.txt and saved cleaned text to cleaned.txt\")\n",
        "    return True\n",
        "\n",
        "def decode_batch(batch, tokenizer):\n",
        "    \"\"\"Decode a batch of tokenized sentences\"\"\"\n",
        "    results = []\n",
        "    for line in batch:\n",
        "        try:\n",
        "            token_ids = list(map(int, line.split()))\n",
        "            decoded_text = tokenizer.decode(token_ids)\n",
        "            results.append(decoded_text)\n",
        "        except ValueError:\n",
        "            # Skip lines that can't be properly converted\n",
        "            continue\n",
        "    return results\n",
        "\n",
        "def segment_3_decode():\n",
        "    \"\"\"\n",
        "    Decodes token IDs back to text using parallel processing.\n",
        "    \"\"\"\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
        "\n",
        "    try:\n",
        "        # Read token IDs\n",
        "        with open(\"encoded.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = [line.strip() for line in f if line.strip()]\n",
        "    except FileNotFoundError:\n",
        "        print(\"encoded.txt not found! Run segment 2 first.\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Decoding {len(lines)} tokenized sentences...\")\n",
        "\n",
        "    # Determine batch size and number of workers\n",
        "    batch_size = 100\n",
        "    max_workers = min(32, (len(lines) // batch_size) + 1)\n",
        "    batches = [lines[i:i+batch_size] for i in range(0, len(lines), batch_size)]\n",
        "\n",
        "    # Process batches in parallel\n",
        "    results = []\n",
        "    decode_func = partial(decode_batch, tokenizer=tokenizer)\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit all batches to the executor\n",
        "        future_to_batch = {executor.submit(decode_func, batch): i for i, batch in enumerate(batches)}\n",
        "\n",
        "        # Process results as they complete with tqdm for progress tracking\n",
        "        for i, future in enumerate(tqdm(concurrent.futures.as_completed(future_to_batch), total=len(batches))):\n",
        "            batch_idx = list(future_to_batch.values())[list(future_to_batch.keys()).index(future)]\n",
        "            batch_results = future.result()\n",
        "            # Store results with their batch index for proper ordering\n",
        "            results.append((batch_idx, batch_results))\n",
        "\n",
        "    # Sort results by batch index to maintain original order\n",
        "    results.sort(key=lambda x: x[0])\n",
        "\n",
        "    # Flatten the sorted results\n",
        "    all_decoded = []\n",
        "    for _, batch_results in results:\n",
        "        all_decoded.extend(batch_results)\n",
        "\n",
        "    # Write results to file\n",
        "    with open(\"decoded.txt\", \"w\", encoding=\"utf-8\") as f_out:\n",
        "        for text in all_decoded:\n",
        "            f_out.write(text + '\\n')\n",
        "\n",
        "    # Verify decoding by comparing with cleaned text\n",
        "    try:\n",
        "        with open(\"cleaned.txt\", \"r\", encoding=\"utf-8\") as f_cleaned:\n",
        "            cleaned_lines = [line.strip() for line in f_cleaned if line.strip()]\n",
        "\n",
        "        # Check if counts match\n",
        "        if len(cleaned_lines) != len(all_decoded):\n",
        "            print(f\"Warning: Number of cleaned lines ({len(cleaned_lines)}) doesn't match decoded lines ({len(all_decoded)})\")\n",
        "        else:\n",
        "            # Calculate simple similarity score\n",
        "            similarity_count = sum(1 for i in range(min(len(cleaned_lines), len(all_decoded)))\n",
        "                                if cleaned_lines[i].lower() == all_decoded[i].lower())\n",
        "            similarity_percentage = (similarity_count / len(cleaned_lines)) * 100\n",
        "            print(f\"Decoding verification: {similarity_percentage:.2f}% similarity with cleaned text\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"cleaned.txt not found. Skipping verification.\")\n",
        "\n",
        "    print(f\"Successfully decoded all tokens to decoded.txt\")\n",
        "    return True\n",
        "\n",
        "def main():\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Ensure all required NLTK resources are available\n",
        "    ensure_nltk_resources()\n",
        "\n",
        "    print(\"SEGMENT 1: Downloading and sentence-splitting a novel...\")\n",
        "    if segment_1_download_and_split():\n",
        "        print(\"\\nSEGMENT 2: Preprocessing and tokenizing sentences...\")\n",
        "        if segment_2_preprocess_and_tokenize():\n",
        "            print(\"\\nSEGMENT 3: Decoding token IDs back to text...\")\n",
        "            segment_3_decode()\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nTotal execution time: {total_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtwDopb5_hoX",
        "outputId": "047d285f-e463-4c8f-91ed-0b8fa0729eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resource punkt is already downloaded.\n",
            "Resource punkt_tab is already downloaded.\n",
            "SEGMENT 1: Downloading and sentence-splitting a novel...\n",
            "Selected book: Frankenstein (ID: 84)\n",
            "Successfully processed 3075 sentences to novel.txt\n",
            "\n",
            "SEGMENT 2: Preprocessing and tokenizing sentences...\n",
            "Processing 8803 sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 89/89 [00:00<00:00, 95618.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully tokenized all sentences to encoded.txt and saved cleaned text to cleaned.txt\n",
            "\n",
            "SEGMENT 3: Decoding token IDs back to text...\n",
            "Decoding 8803 tokenized sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 89/89 [00:00<00:00, 122915.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoding verification: 100.00% similarity with cleaned text\n",
            "Successfully decoded all tokens to decoded.txt\n",
            "\n",
            "Total execution time: 3.61 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the optimized code I provided, cleaned.txt is a new intermediate file that was added to help track and debug the text processing pipeline. Let me explain its purpose and content:\n",
        "What is cleaned.txt?\n",
        "cleaned.txt is a file that contains the \"cleaned\" version of each sentence from the original novel before tokenization occurs. By \"cleaned,\" I mean the text after all the preprocessing steps have been applied but before it's converted to token IDs.\n",
        "What preprocessing is applied to create cleaned.txt?\n",
        "Each sentence undergoes the following transformations:\n",
        "\n",
        "Lowercase conversion: All text is converted to lowercase\n",
        "Character filtering: Everything except lowercase letters (a-z) and spaces is removed\n",
        "Space normalization: Multiple spaces are condensed into a single space, and leading/trailing spaces are trimmed\n",
        "\n",
        "For example, a sentence like:\n",
        "\"Hello, world! How are you doing today?\"\n",
        "Would become:\n",
        "\"hello world how are you doing today\"\n",
        "Why was cleaned.txt added?\n",
        "Adding this file serves several important purposes:\n",
        "\n",
        "Debugging aid: It allows you to see exactly what text is being tokenized, making it easier to identify issues in the process\n",
        "Verification reference: The decoded output should match this cleaned text (if the tokenization and decoding are working correctly)\n",
        "Order preservation: By keeping this file, we can verify that the order of sentences is preserved throughout the parallel processing pipeline\n",
        "Quality check: The code now includes a verification step that compares the decoded text with the cleaned text and reports a similarity percentage\n",
        "\n",
        "How is cleaned.txt used in the process?\n",
        "\n",
        "During segment_2_preprocess_and_tokenize(), each sentence is cleaned and its cleaned version is saved alongside its token IDs\n",
        "Both versions are written to their respective files (cleaned.txt and encoded.txt), maintaining the same order\n",
        "In segment_3_decode(), after decoding all token IDs back to text, the code compares the decoded text with the cleaned text to verify accuracy\n",
        "\n",
        "This intermediate file helps ensure that what goes into the tokenization process matches what comes out of the decoding process, providing a way to validate that the entire pipeline is working correctly. If there's a significant mismatch between cleaned.txt and decoded.txt, it would indicate a problem in the tokenization/detokenization process."
      ],
      "metadata": {
        "id": "fvnE-f5yDKRV"
      }
    }
  ]
}